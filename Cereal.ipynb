{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- For everybody to be able to follow along and learn something\n",
    "- Use vanilla python with standard libraries only when needed\n",
    "\n",
    "## Disclamers\n",
    "- Not necessarily the best way or even the right way\n",
    "- \n",
    "\n",
    "API key:\n",
    "kqA964yzjSblTKwZ3Fa9z3GMU8rCZO4fLVXTrZuu\n",
    "example: https://developer.nrel.gov/api/alt-fuel-stations/v1/nearest.json?api_key=kqA964yzjSblTKwZ3Fa9z3GMU8rCZO4fLVXTrZuu&location=Denver+CO\n",
    "\n",
    "Helpful links: \n",
    "- http://nymag.com/restaurants/features/breakfast/47390/index1.html\n",
    "- http://www.asaurus.net/~buhr/academic/2004-2-ubc-stat445/projects/project1-sample-2.pdf\n",
    "- http://www.lavasurfer.com/boxtop/boxtop-cerealratings.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Prepare our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV into our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- name: Name of cereal\n",
    "- mfr: Manufacturer of cereal\n",
    "    - A = American Home Food Products;\n",
    "    - G = General Mills\n",
    "    - K = Kelloggs\n",
    "    - N = Nabisco\n",
    "    - P = Post\n",
    "    - Q = Quaker Oats\n",
    "    - R = Ralston Purina\n",
    "- type: cold vs hot\n",
    "- calories: calories per serving\n",
    "- protein: grams of protein\n",
    "- fat: grams of fat\n",
    "- sodium: milligrams of sodium\n",
    "- fiber: grams of dietary fiber\n",
    "- carbo: grams of complex carbohydrates\n",
    "- sugars: grams of sugars\n",
    "- potass: milligrams of potassium\n",
    "- vitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended serving\n",
    "- shelf: display shelf (1, 2, or 3, counting from the floor)\n",
    "- weight: weight in ounces of one serving\n",
    "- cups: number of cups in one serving\n",
    "- rating: a rating of the cereals (Possibly from Consumer Reports?)\n",
    "\n",
    "Source: https://www.kaggle.com/crawford/80-cereals\n",
    "\n",
    "[View this File](/edit/cereal.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber', 'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups', 'rating'], ['100% Bran', 'N', 'C', '70', '4', '1', '130', '10', '5', '6', '280', '25', '3', '1', '0.33', '68.402973'], ['100% Natural Bran', 'Q', 'C', '120', '3', '5', '15', '2', '8', '8', '135', '0', '3', '1', '1', '33.983679'], ['All-Bran', 'K', 'C', '70', '4', '1', '260', '9', '7', '5', '320', '25', '3', '1', '0.33', '59.425505'], ['All-Bran with Extra Fiber', 'K', 'C', '50', '4', '0', '140', '14', '8', '0', '330', '25', '3', '1', '0.5', '93.704912'], ['Almond Delight', 'R', 'C', '110', '2', '2', '200', '1', '14', '8', '0', '25', '3', '1', '0.75', '34.384843'], ['Apple Cinnamon Cheerios', 'G', 'C', '110', '2', '2', '180', '1.5', '10.5', '10', '70', '25', '1', '1', '0.75', '29.509541'], ['Apple Jacks', 'K', 'C', '110', '2', '0', '125', '1', '11', '14', '30', '25', '2', '1', '1', '33.174094'], ['Basic 4', 'G', 'C', '130', '3', '2', '210', '2', '18', '8', '100', '25', '3', '1.33', '0.75', '37.038562'], ['Bran Chex', 'R', 'C', '90', '2', '1', '200', '4', '15', '6', '125', '25', '1', '1', '0.67', '49.120253'], ['Bran Flakes', 'P', 'C', '90', '3', '0', '210', '5', '13', '5', '190', '25', '3', '1', '0.67', '53.313813'], [\"Cap'n'Crunch\", 'Q', 'C', '120', '1', '2', '220', '0', '12', '12', '35', '25', '2', '1', '0.75', '18.042851'], ['Cheerios', 'G', 'C', '110', '6', '2', '290', '2', '17', '1', '105', '25', '1', '1', '1.25', '50.764999'], ['Cinnamon Toast Crunch', 'G', 'C', '120', '1', '3', '210', '0', '13', '9', '45', '25', '2', '1', '0.75', '19.823573'], ['Clusters', 'G', 'C', '110', '3', '2', '140', '2', '13', '7', '105', '25', '3', '1', '0.5', '40.400208'], ['Cocoa Puffs', 'G', 'C', '110', '1', '1', '180', '0', '12', '13', '55', '25', '2', '1', '1', '22.736446'], ['Corn Chex', 'R', 'C', '110', '2', '0', '280', '0', '22', '3', '25', '25', '1', '1', '1', '41.445019'], ['Corn Flakes', 'K', 'C', '100', '2', '0', '290', '1', '21', '2', '35', '25', '1', '1', '1', '45.863324'], ['Corn Pops', 'K', 'C', '110', '1', '0', '90', '1', '13', '12', '20', '25', '2', '1', '1', '35.782791'], ['Count Chocula', 'G', 'C', '110', '1', '1', '180', '0', '12', '13', '65', '25', '2', '1', '1', '22.396513'], [\"Cracklin' Oat Bran\", 'K', 'C', '110', '3', '3', '140', '4', '10', '7', '160', '25', '3', '1', '0.5', '40.448772'], ['Cream of Wheat (Quick)', 'N', 'H', '100', '3', '0', '80', '1', '21', '0', '0', '0', '2', '1', '1', '64.533816'], ['Crispix', 'K', 'C', '110', '2', '0', '220', '1', '21', '3', '30', '25', '3', '1', '1', '46.895644'], ['Crispy Wheat & Raisins', 'G', 'C', '100', '2', '1', '140', '2', '11', '10', '120', '25', '3', '1', '0.75', '36.176196'], ['Double Chex', 'R', 'C', '100', '2', '0', '190', '1', '18', '5', '80', '25', '3', '1', '0.75', '44.330856'], ['Froot Loops', 'K', 'C', '110', '2', '1', '125', '1', '11', '13', '30', '25', '2', '1', '1', '32.207582'], ['Frosted Flakes', 'K', 'C', '110', '1', '0', '200', '1', '14', '11', '25', '25', '1', '1', '0.75', '31.435973'], ['Frosted Mini-Wheats', 'K', 'C', '100', '3', '0', '0', '3', '14', '7', '100', '25', '2', '1', '0.8', '58.345141'], ['Fruit & Fibre Dates; Walnuts; and Oats', 'P', 'C', '120', '3', '2', '160', '5', '12', '10', '200', '25', '3', '1.25', '0.67', '40.917047'], ['Fruitful Bran', 'K', 'C', '120', '3', '0', '240', '5', '14', '12', '190', '25', '3', '1.33', '0.67', '41.015492'], ['Fruity Pebbles', 'P', 'C', '110', '1', '1', '135', '0', '13', '12', '25', '25', '2', '1', '0.75', '28.025765'], ['Golden Crisp', 'P', 'C', '100', '2', '0', '45', '0', '11', '15', '40', '25', '1', '1', '0.88', '35.252444'], ['Golden Grahams', 'G', 'C', '110', '1', '1', '280', '0', '15', '9', '45', '25', '2', '1', '0.75', '23.804043'], ['Grape Nuts Flakes', 'P', 'C', '100', '3', '1', '140', '3', '15', '5', '85', '25', '3', '1', '0.88', '52.076897'], ['Grape-Nuts', 'P', 'C', '110', '3', '0', '170', '3', '17', '3', '90', '25', '3', '1', '0.25', '53.371007'], ['Great Grains Pecan', 'P', 'C', '120', '3', '3', '75', '3', '13', '4', '100', '25', '3', '1', '0.33', '45.811716'], ['Honey Graham Ohs', 'Q', 'C', '120', '1', '2', '220', '1', '12', '11', '45', '25', '2', '1', '1', '21.871292'], ['Honey Nut Cheerios', 'G', 'C', '110', '3', '1', '250', '1.5', '11.5', '10', '90', '25', '1', '1', '0.75', '31.072217'], ['Honey-comb', 'P', 'C', '110', '1', '0', '180', '0', '14', '11', '35', '25', '1', '1', '1.33', '28.742414'], ['Just Right Crunchy  Nuggets', 'K', 'C', '110', '2', '1', '170', '1', '17', '6', '60', '100', '3', '1', '1', '36.523683'], ['Just Right Fruit & Nut', 'K', 'C', '140', '3', '1', '170', '2', '20', '9', '95', '100', '3', '1.3', '0.75', '36.471512'], ['Kix', 'G', 'C', '110', '2', '1', '260', '0', '21', '3', '40', '25', '2', '1', '1.5', '39.241114'], ['Life', 'Q', 'C', '100', '4', '2', '150', '2', '12', '6', '95', '25', '2', '1', '0.67', '45.328074'], ['Lucky Charms', 'G', 'C', '110', '2', '1', '180', '0', '12', '12', '55', '25', '2', '1', '1', '26.734515'], ['Maypo', 'A', 'H', '100', '4', '1', '0', '0', '16', '3', '95', '25', '2', '1', '1', '54.850917'], ['Muesli Raisins; Dates; & Almonds', 'R', 'C', '150', '4', '3', '95', '3', '16', '11', '170', '25', '3', '1', '1', '37.136863'], ['Muesli Raisins; Peaches; & Pecans', 'R', 'C', '150', '4', '3', '150', '3', '16', '11', '170', '25', '3', '1', '1', '34.139765'], ['Mueslix Crispy Blend', 'K', 'C', '160', '3', '2', '150', '3', '17', '13', '160', '25', '3', '1.5', '0.67', '30.313351'], ['Multi-Grain Cheerios', 'G', 'C', '100', '2', '1', '220', '2', '15', '6', '90', '25', '1', '1', '1', '40.105965'], ['Nut&Honey Crunch', 'K', 'C', '120', '2', '1', '190', '0', '15', '9', '40', '25', '2', '1', '0.67', '29.924285'], ['Nutri-Grain Almond-Raisin', 'K', 'C', '140', '3', '2', '220', '3', '21', '7', '130', '25', '3', '1.33', '0.67', '40.69232'], ['Nutri-grain Wheat', 'K', 'C', '90', '3', '0', '170', '3', '18', '2', '90', '25', '3', '1', '1', '59.642837'], ['Oatmeal Raisin Crisp', 'G', 'C', '130', '3', '2', '170', '1.5', '13.5', '10', '120', '25', '3', '1.25', '0.5', '30.450843'], ['Post Nat. Raisin Bran', 'P', 'C', '120', '3', '1', '200', '6', '11', '14', '260', '25', '3', '1.33', '0.67', '37.840594'], ['Product 19', 'K', 'C', '100', '3', '0', '320', '1', '20', '3', '45', '100', '3', '1', '1', '41.50354'], ['Puffed Rice', 'Q', 'C', '50', '1', '0', '0', '0', '13', '0', '15', '0', '3', '0.5', '1', '60.756112'], ['Puffed Wheat', 'Q', 'C', '50', '2', '0', '0', '1', '10', '0', '50', '0', '3', '0.5', '1', '63.005645'], ['Quaker Oat Squares', 'Q', 'C', '100', '4', '1', '135', '2', '14', '6', '110', '25', '3', '1', '0.5', '49.511874'], ['Quaker Oatmeal', 'Q', 'H', '100', '5', '2', '0', '2.7', '-1', '-1', '110', '0', '1', '1', '0.67', '50.828392'], ['Raisin Bran', 'K', 'C', '120', '3', '1', '210', '5', '14', '12', '240', '25', '2', '1.33', '0.75', '39.259197'], ['Raisin Nut Bran', 'G', 'C', '100', '3', '2', '140', '2.5', '10.5', '8', '140', '25', '3', '1', '0.5', '39.7034'], ['Raisin Squares', 'K', 'C', '90', '2', '0', '0', '2', '15', '6', '110', '25', '3', '1', '0.5', '55.333142'], ['Rice Chex', 'R', 'C', '110', '1', '0', '240', '0', '23', '2', '30', '25', '1', '1', '1.13', '41.998933'], ['Rice Krispies', 'K', 'C', '110', '2', '0', '290', '0', '22', '3', '35', '25', '1', '1', '1', '40.560159'], ['Shredded Wheat', 'N', 'C', '80', '2', '0', '0', '3', '16', '0', '95', '0', '1', '0.83', '1', '68.235885'], [\"Shredded Wheat 'n'Bran\", 'N', 'C', '90', '3', '0', '0', '4', '19', '0', '140', '0', '1', '1', '0.67', '74.472949'], ['Shredded Wheat spoon size', 'N', 'C', '90', '3', '0', '0', '3', '20', '0', '120', '0', '1', '1', '0.67', '72.801787'], ['Smacks', 'K', 'C', '110', '2', '1', '70', '1', '9', '15', '40', '25', '2', '1', '0.75', '31.230054'], ['Special K', 'K', 'C', '110', '6', '0', '230', '1', '16', '3', '55', '25', '1', '1', '1', '53.131324'], ['Strawberry Fruit Wheats', 'N', 'C', '90', '2', '0', '15', '3', '15', '5', '90', '25', '2', '1', '1', '59.363993'], ['Total Corn Flakes', 'G', 'C', '110', '2', '1', '200', '0', '21', '3', '35', '100', '3', '1', '1', '38.839746'], ['Total Raisin Bran', 'G', 'C', '140', '3', '1', '190', '4', '15', '14', '230', '100', '3', '1.5', '1', '28.592785'], ['Total Whole Grain', 'G', 'C', '100', '3', '1', '200', '3', '16', '3', '110', '100', '3', '1', '1', '46.658844'], ['Triples', 'G', 'C', '110', '2', '1', '250', '0', '21', '3', '60', '25', '3', '1', '0.75', '39.106174'], ['Trix', 'G', 'C', '110', '1', '1', '140', '0', '13', '12', '25', '25', '2', '1', '1', '27.753301'], ['Wheat Chex', 'R', 'C', '100', '3', '1', '230', '3', '17', '3', '115', '25', '1', '1', '0.67', '49.787445'], ['Wheaties', 'G', 'C', '100', '3', '1', '200', '3', '17', '3', '110', '25', '1', '1', '1', '51.592193'], ['Wheaties Honey Gold', 'G', 'C', '110', '2', '1', '200', '1', '16', '8', '60', '25', '1', '1', '0.75', '36.187559']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('cereal.csv') as f:\n",
    "    data = list(csv.reader(f))\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows:  78\n",
      "Number of Cols:  16\n"
     ]
    }
   ],
   "source": [
    "print('Number of Rows: ', len(data))\n",
    "print('Number of Cols: ', len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name   mfr    type   calori protei fat    sodium fiber  carbo  sugars potass vitami shelf  weight cups   rating \n",
      "\n",
      "100% B N      C      70     4      1      130    10     5      6      280    25     3      1      0.33   68.402 \n",
      "\n",
      "100% N Q      C      120    3      5      15     2      8      8      135    0      3      1      1      33.983 \n",
      "\n",
      "All-Br K      C      70     4      1      260    9      7      5      320    25     3      1      0.33   59.425 \n",
      "\n",
      "All-Br K      C      50     4      0      140    14     8      0      330    25     3      1      0.5    93.704 \n",
      "\n",
      "Almond R      C      110    2      2      200    1      14     8      0      25     3      1      0.75   34.384 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  78 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def peak_in(d, rows=6):\n",
    "    for row in d[0:rows]:\n",
    "        for col_num, col in enumerate(row):\n",
    "            if (col_num <= 15):\n",
    "                print(str(col)[:6], end=(' ' * (7 - len(str(col)[:6]))))\n",
    "            else:\n",
    "                print(' ... ')\n",
    "                break\n",
    "        print('\\n')\n",
    "\n",
    "    print('** Total Number of columns: ', str(len(data[0])))\n",
    "    print('** Displaying rows 1 - ', str(rows), ' of ', str(len(d)), '\\n')\n",
    "\n",
    "peak_in(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Randomize Our Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% B N      C      70     4      1      130    10     5      6      280    25     3      1      0.33   68.402 \n",
      "\n",
      "100% N Q      C      120    3      5      15     2      8      8      135    0      3      1      1      33.983 \n",
      "\n",
      "All-Br K      C      70     4      1      260    9      7      5      320    25     3      1      0.33   59.425 \n",
      "\n",
      "All-Br K      C      50     4      0      140    14     8      0      330    25     3      1      0.5    93.704 \n",
      "\n",
      "Almond R      C      110    2      2      200    1      14     8      0      25     3      1      0.75   34.384 \n",
      "\n",
      "Apple  G      C      110    2      2      180    1.5    10.5   10     70     25     1      1      0.75   29.509 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_randomized = data[1:]\n",
    "import random\n",
    "# random.shuffle(data_randomized);\n",
    "\n",
    "peak_in(data_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0   4.0    1.0    130.0  10.0   5.0    6.0    280.0  25.0   1.0    0.33   68.402 \n",
      "\n",
      "120.0  3.0    5.0    15.0   2.0    8.0    8.0    135.0  0.0    1.0    1.0    33.983 \n",
      "\n",
      "70.0   4.0    1.0    260.0  9.0    7.0    5.0    320.0  25.0   1.0    0.33   59.425 \n",
      "\n",
      "50.0   4.0    0.0    140.0  14.0   8.0    0.0    330.0  25.0   1.0    0.5    93.704 \n",
      "\n",
      "110.0  2.0    2.0    200.0  1.0    14.0   8.0    0.0    25.0   1.0    0.75   34.384 \n",
      "\n",
      "110.0  2.0    2.0    180.0  1.5    10.5   10.0   70.0   25.0   1.0    0.75   29.509 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_values_only = []\n",
    "\n",
    "for row_num, row in enumerate(data_randomized):\n",
    "    data_values_only.append([])\n",
    "    for col_num, col in enumerate(row):\n",
    "        if (col_num not in {0, 1, 2, 12}):\n",
    "            data_values_only[row_num].append(float(col))\n",
    "            \n",
    "peak_in(data_values_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names for reference:\n",
    "\n",
    "(0) Cals | (1) Prot | (2) Fat | (3) Na | (4) Fib | (5) Carb | (6) Sug | (7) K  | (8) Vit | (9)  Weight | (10) Cups | (11) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0   4.0    1.0    130.0  10.0   5.0    6.0    280.0  25.0   0.33   68.402 \n",
      "\n",
      "120.0  3.0    5.0    15.0   2.0    8.0    8.0    135.0  0.0    1.0    33.983 \n",
      "\n",
      "70.0   4.0    1.0    260.0  9.0    7.0    5.0    320.0  25.0   0.33   59.425 \n",
      "\n",
      "50.0   4.0    0.0    140.0  14.0   8.0    0.0    330.0  25.0   0.5    93.704 \n",
      "\n",
      "110.0  2.0    2.0    200.0  1.0    14.0   8.0    0.0    25.0   0.75   34.384 \n",
      "\n",
      "110.0  2.0    2.0    180.0  1.5    10.5   10.0   70.0   25.0   0.75   29.509 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_proportional = [];\n",
    "\n",
    "for row_num, row in enumerate(data_values_only):\n",
    "    data_proportional.append([])\n",
    "    for col_num, col in enumerate(row):\n",
    "        if (col_num not in {9, 11}):\n",
    "            data_proportional[row_num].append(col / data_values_only[row_num][9])\n",
    "        elif (col_num == 11):\n",
    "            data_proportional[row_num].append(col) # Score, unchanged\n",
    "\n",
    "peak_in(data_proportional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names for reference:\n",
    "(0) Cals | (1) Prot | (2) Fat | (3) Na | (4) Fib | (5) Carb | (6) Sug | (7) K | (8) Vit | (9) Weight | (10) Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Y-Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      70.0   4.0    1.0    130.0  10.0   5.0    6.0    280.0  25.0   0.33   68.402 \n",
      "\n",
      "1      120.0  3.0    5.0    15.0   2.0    8.0    8.0    135.0  0.0    1.0    33.983 \n",
      "\n",
      "1      70.0   4.0    1.0    260.0  9.0    7.0    5.0    320.0  25.0   0.33   59.425 \n",
      "\n",
      "1      50.0   4.0    0.0    140.0  14.0   8.0    0.0    330.0  25.0   0.5    93.704 \n",
      "\n",
      "1      110.0  2.0    2.0    200.0  1.0    14.0   8.0    0.0    25.0   0.75   34.384 \n",
      "\n",
      "1      110.0  2.0    2.0    180.0  1.5    10.5   10.0   70.0   25.0   0.75   29.509 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_prepped = [];\n",
    "\n",
    "for row_num, row in enumerate(data_proportional):\n",
    "    data_prepped.append([1] + data_proportional[row_num])\n",
    "            \n",
    "peak_in(data_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Our Data into two parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Labels for Reference\n",
    "\n",
    "(0) Y-int | (1) Cals | (2) Prot | (3) Fat | (4) Na | (5) Fib | (6) Carb | (7) Sug | (8) K | (9) Vit | (10) Weight | (11) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 61\n",
      "Number of validation examples: 16\n",
      "Number of features:  11\n"
     ]
    }
   ],
   "source": [
    "num_of_training_examples = int(len(data_prepped)*0.8)\n",
    "\n",
    "training_set   = data_prepped[:num_of_training_examples]\n",
    "validation_set = data_prepped[num_of_training_examples:]\n",
    "\n",
    "print('Number of training examples: ' + str(len(training_set)))\n",
    "print('Number of validation examples: ' + str(len(validation_set)))\n",
    "print('Number of features: ', len(validation_set[0]) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create Our Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Our Parameters For Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (Sample): \n",
      "1      70.0   4.0    1.0    130.0  10.0   5.0    6.0    280.0  25.0   0.33   \n",
      "\n",
      "1      120.0  3.0    5.0    15.0   2.0    8.0    8.0    135.0  0.0    1.0    \n",
      "\n",
      "1      70.0   4.0    1.0    260.0  9.0    7.0    5.0    320.0  25.0   0.33   \n",
      "\n",
      "1      50.0   4.0    0.0    140.0  14.0   8.0    0.0    330.0  25.0   0.5    \n",
      "\n",
      "1      110.0  2.0    2.0    200.0  1.0    14.0   8.0    0.0    25.0   0.75   \n",
      "\n",
      "1      110.0  2.0    2.0    180.0  1.5    10.5   10.0   70.0   25.0   0.75   \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  61 \n",
      "\n",
      "---\n",
      "\n",
      "y:  [68.402973, 33.983679, 59.425505, 93.704912, 34.384843, 29.509541, 33.174094, 37.038562, 49.120253, 53.313813, 18.042851, 50.764999, 19.823573, 40.400208, 22.736446, 41.445019, 45.863324, 35.782791, 22.396513, 40.448772, 64.533816, 46.895644, 36.176196, 44.330856, 32.207582, 31.435973, 58.345141, 40.917047, 41.015492, 28.025765, 35.252444, 23.804043, 52.076897, 53.371007, 45.811716, 21.871292, 31.072217, 28.742414, 36.523683, 36.471512, 39.241114, 45.328074, 26.734515, 54.850917, 37.136863, 34.139765, 30.313351, 40.105965, 29.924285, 40.69232, 59.642837, 30.450843, 37.840594, 41.50354, 60.756112, 63.005645, 49.511874, 50.828392, 39.259197, 39.7034, 55.333142] \n",
      "\n",
      "---\n",
      "\n",
      "m:  61 \n",
      "\n",
      "---\n",
      "\n",
      "theta:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for row in training_set:\n",
    "    X.append(row[:11])\n",
    "    y.append(row[11])\n",
    "    \n",
    "m = len(X)\n",
    "theta = [0] * m\n",
    "alpha = 0.3\n",
    "iterations = 5\n",
    "\n",
    "print('X (Sample): ')\n",
    "peak_in(X)\n",
    "print('---\\n')\n",
    "print('y: ', y, '\\n\\n---\\n')\n",
    "print('m: ', m, '\\n\\n---\\n')\n",
    "print('theta: ', theta, '\\n\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      1      1      1      1      1      1      1      1      1      1      1      1      1      1      1       ... \n",
      "\n",
      "\n",
      "70.0   120.0  70.0   50.0   110.0  110.0  110.0  97.744 90.0   90.0   120.0  110.0  120.0  110.0  110.0  110.0   ... \n",
      "\n",
      "\n",
      "4.0    3.0    4.0    4.0    2.0    2.0    2.0    2.2556 2.0    3.0    1.0    6.0    1.0    3.0    1.0    2.0     ... \n",
      "\n",
      "\n",
      "1.0    5.0    1.0    0.0    2.0    2.0    0.0    1.5037 1.0    0.0    2.0    2.0    3.0    2.0    1.0    0.0     ... \n",
      "\n",
      "\n",
      "130.0  15.0   260.0  140.0  200.0  180.0  125.0  157.89 200.0  210.0  220.0  290.0  210.0  140.0  180.0  280.0   ... \n",
      "\n",
      "\n",
      "10.0   2.0    9.0    14.0   1.0    1.5    1.0    1.5037 4.0    5.0    0.0    2.0    0.0    2.0    0.0    0.0     ... \n",
      "\n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  11 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transpose(matrix):\n",
    "    return list(map(list, zip(*matrix)))\n",
    "\n",
    "example_matrix = [[1,2,3],\n",
    "                  [4,5,6],\n",
    "                  [7,8,9]]\n",
    "\n",
    "peak_in(transpose(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize All the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refresher of statistics terms:\n",
    "\n",
    "- **μ or mu:** The mean (i.e. average) of a list of values.  \n",
    "\n",
    "- **σ or sigma or standard deviation:** A measure of how dispersed a data set is from it's mean.\n",
    "\n",
    "- **range**: Difference between the highest and lowest item in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      70.0   4.0    1.0    130.0  10.0   5.0    6.0    280.0  25.0   0.33   \n",
      "\n",
      "1      120.0  3.0    5.0    15.0   2.0    8.0    8.0    135.0  0.0    1.0    \n",
      "\n",
      "1      70.0   4.0    1.0    260.0  9.0    7.0    5.0    320.0  25.0   0.33   \n",
      "\n",
      "1      50.0   4.0    0.0    140.0  14.0   8.0    0.0    330.0  25.0   0.5    \n",
      "\n",
      "1      110.0  2.0    2.0    200.0  1.0    14.0   8.0    0.0    25.0   0.75   \n",
      "\n",
      "1      110.0  2.0    2.0    180.0  1.5    10.5   10.0   70.0   25.0   0.75   \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  61 \n",
      "\n",
      "[70.0, 120.0, 70.0, 50.0, 110.0, 110.0, 110.0, 97.74436090225564, 90.0, 90.0, 120.0, 110.0, 120.0, 110.0, 110.0, 110.0, 100.0, 110.0, 110.0, 110.0, 100.0, 110.0, 100.0, 100.0, 110.0, 110.0, 100.0, 96.0, 90.22556390977444, 110.0, 100.0, 110.0, 100.0, 110.0, 120.0, 120.0, 110.0, 110.0, 110.0, 107.6923076923077, 110.0, 100.0, 110.0, 100.0, 150.0, 150.0, 106.66666666666667, 100.0, 120.0, 105.26315789473684, 90.0, 104.0, 90.22556390977444, 100.0, 100.0, 100.0, 100.0, 100.0, 90.22556390977444, 100.0, 90.0]\n",
      "mu: [1, 104.39415057189001, 2.498129308137936, 1.0789154475090628, 154.37375911405246, 2.126687462666755, 13.70925516818528, 7.089169961473671, 94.22086742707968, 25.452044360586207, 0.8086670079991909] \n",
      "\n",
      "range: [0, 100.0, 5.0, 5.0, 320.0, 14.0, 27.0, 16.0, 330.0, 100.0, 1.75] \n",
      "\n",
      "Normalized X:\n",
      "1      -0.343 0.3003 -0.015 -0.076 0.5623 -0.322 -0.068 0.5629 -0.004 -0.273 \n",
      "\n",
      "1      0.1560 0.1003 0.7842 -0.435 -0.009 -0.211 0.0569 0.1235 -0.254 0.1093 \n",
      "\n",
      "1      -0.343 0.3003 -0.015 0.3300 0.4909 -0.248 -0.130 0.6841 -0.004 -0.273 \n",
      "\n",
      "1      -0.543 0.3003 -0.215 -0.044 0.8480 -0.211 -0.443 0.7144 -0.004 -0.176 \n",
      "\n",
      "1      0.0560 -0.099 0.1842 0.1425 -0.080 0.0107 0.0569 -0.285 -0.004 -0.033 \n",
      "\n",
      "1      0.0560 -0.099 0.1842 0.0800 -0.044 -0.118 0.1819 -0.073 -0.004 -0.033 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  61 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "peak_in(X)\n",
    "\n",
    "# returns a list of means for each feature (the number of features)\n",
    "mu = list(map(statistics.mean,transpose(X)))\n",
    "\n",
    "# returns list of ranges for each feature\n",
    "print(transpose(X)[1])\n",
    "\n",
    "X_range = list(map(lambda feature: max(feature) - min(feature), transpose(X))) \n",
    "\n",
    "# returns a normalized version of X\n",
    "def normalize(X):\n",
    "    normalized_X = []\n",
    "    for row_num, row in enumerate(X):\n",
    "        normalized_X.append([])\n",
    "        for col_num in range(len(row)):\n",
    "            if (col_num == 0): # y-intercept does not need normalization\n",
    "                normalized_X[row_num].append(row[col_num])\n",
    "            else:\n",
    "                normalized_X[row_num].append(\n",
    "                    (row[col_num] - mu[col_num]) / X_range[col_num]\n",
    "                )\n",
    "    return normalized_X\n",
    "\n",
    "X_norm = normalize(X)\n",
    "\n",
    "print('mu:', mu, '\\n')\n",
    "print('range:', X_range, '\\n')\n",
    "print('Normalized X:')\n",
    "peak_in(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J(θ1)=12m∑i=1m(hθ(x(i))−y(i))2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-33.983679\n",
      "all_errors [-68.402973, -33.983679, -59.425505, -93.704912, -34.384843, -29.509541, -33.174094, -37.038562, -49.120253, -53.313813, -18.042851, -50.764999, -19.823573, -40.400208, -22.736446, -41.445019, -45.863324, -35.782791, -22.396513, -40.448772, -64.533816, -46.895644, -36.176196, -44.330856, -32.207582, -31.435973, -58.345141, -40.917047, -41.015492, -28.025765, -35.252444, -23.804043, -52.076897, -53.371007, -45.811716, -21.871292, -31.072217, -28.742414, -36.523683, -36.471512, -39.241114, -45.328074, -26.734515, -54.850917, -37.136863, -34.139765, -30.313351, -40.105965, -29.924285, -40.69232, -59.642837, -30.450843, -37.840594, -41.50354, -60.756112, -63.005645, -49.511874, -50.828392, -39.259197, -39.7034, -55.333142]\n",
      "948.18139426414\n"
     ]
    }
   ],
   "source": [
    "def hypothesis(theta, X_row):\n",
    "    # X_row = a single row of X...  \n",
    "    # theta = all theta values... currently [0, 0, 0, ... 0]\n",
    "    return sum([theta*column for theta, column in zip(X_row, theta)])\n",
    "\n",
    "def individual_trial_error(theta, X_row, correct_answer):\n",
    "    # correct_answer = the corresponding y value for the current X_row\n",
    "    return hypothesis(theta, X_row) - correct_answer\n",
    "\n",
    "print (individual_trial_error(theta, X_norm[1], y[1]))\n",
    "theta = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "def all_errors(theta, X, correct_answers):\n",
    "    # X = all rows of X\n",
    "    # theta = all theta values\n",
    "    # correct_answers = all y values for all rows of X\n",
    "    return [individual_trial_error(theta, X[row], y[row]) for row in range(len(X))]\n",
    "\n",
    "\n",
    "print ('all_errors', all_errors(theta, X, y))\n",
    "    \n",
    "def squared_error(theta, X, correct_answers):\n",
    "    return sum([individual_trial_error(theta, X[row], y[row])**2 for row in range(len(X))])\n",
    "\n",
    "# Our goal is to reduce this!\n",
    "def loss_function(theta, X, correct_answers, m):\n",
    "    return (1/(2*m))*squared_error(theta, X, correct_answers)\n",
    "\n",
    "print(loss_function(theta, X_norm, y, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Apply Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def partial_derivative_of_loss_function(theta, X, y, col):\n",
    "#     print('losing it: ', print(all_errors(theta, X_norm, y)[0]))\n",
    "#     print('printing this:')\n",
    "#     print(X[2][1])\n",
    "#     print('will I figure it out')\n",
    "#     print([all_errors(theta, X_norm, y)[row]*X[row][col] for row in range(len(X))])\n",
    "#     print('\\n\\n')\n",
    "#     print('all_errors:', all_errors(theta, X_norm, y))\n",
    "#     print('\\n\\n')\n",
    "#     print ('before the sum: ', [all_errors(theta, X_norm, y)[row]*X[row][col] for row in range(len(X))])\n",
    "#     print('partial derivative: ', sum([all_errors(theta, X_norm, y)[row]*X[row][col] for row in range(len(X))]))\n",
    "    return sum([all_errors(theta, X_norm, y)[row]*X[row][col] for row in range(len(X))])\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(theta, X, y, alpha, m):\n",
    "    new_theta = []\n",
    "#     print('theta before:', theta)\n",
    "#     print('alpha*1/m', alpha*(1/m))\n",
    "#     print('m', m)\n",
    "    for theta_index, theta_value in enumerate(theta):\n",
    "        new_theta.append(theta_value)\n",
    "        new_theta[theta_index] = theta_value - (alpha*(1/m)*partial_derivative_of_loss_function(theta, X, y, theta_index))\n",
    "\n",
    "    return new_theta\n",
    "\n",
    "\n",
    "# theta = gradient_descent(theta, X_norm, y, alpha, m)\n",
    "# print('finally...theta is:', some_theta)\n",
    "# print('these should be old:', theta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gradient_descent(X_norm, theta, y)\n",
    "# print(gradient_descent(X_norm, theta, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505.86698537250027\n",
      "286.73237405361766\n",
      "177.11448519751784\n",
      "121.30755796031703\n",
      "92.00567509739906\n",
      "75.81954834673017\n",
      "66.1796931557196\n",
      "59.858937784600506\n",
      "55.26840959212992\n",
      "51.62252823702687\n",
      "48.529815140664006\n",
      "45.79235558272627\n",
      "43.30750305947955\n",
      "41.01968464466181\n",
      "38.896757970056974\n",
      "36.918400474587926\n",
      "35.07039629747636\n",
      "33.34181471743958\n",
      "31.72360702741899\n",
      "30.207899907102558\n",
      "28.787631438393056\n",
      "27.456356277224003\n",
      "26.20813488264523\n",
      "25.03746502054786\n",
      "23.93923499054288\n",
      "22.908688433410642\n",
      "21.94139568213308\n",
      "21.033229126113643\n",
      "20.18034129091873\n",
      "19.379144944084395\n",
      "18.626294839423252\n",
      "17.918670863781056\n",
      "17.253362427762877\n",
      "16.62765398304276\n",
      "16.03901157187083\n",
      "15.485070328314091\n",
      "14.96362286005372\n",
      "14.472608446393407\n",
      "14.010102993600619\n",
      "13.574309693345931\n",
      "13.163550334103604\n",
      "12.776257219074902\n",
      "12.410965647577138\n",
      "12.06630691995276\n",
      "11.74100182892765\n",
      "11.43385460300865\n",
      "11.143747269976696\n",
      "10.869634410817902\n",
      "10.61053827655626\n",
      "10.365544242418698\n",
      "10.133796575588894\n",
      "9.91449449450011\n",
      "9.706888499189239\n",
      "9.510276953692514\n",
      "9.32400290281754\n",
      "9.147451106882428\n",
      "8.980045279178837\n",
      "8.821245511998237\n",
      "8.670545878065163\n",
      "8.527472195153917\n",
      "8.391579942530909\n",
      "8.26245231866835\n",
      "8.13969843042126\n",
      "8.022951604552478\n",
      "7.911867813133671\n",
      "7.806124204947242\n",
      "7.7054177355691476\n",
      "7.60946388932709\n",
      "7.51799548680739\n",
      "7.430761572027534\n",
      "7.347526373804479\n",
      "7.268068336231567\n",
      "7.19217921353319\n",
      "7.1196632248970815\n",
      "7.050336265190997\n",
      "6.984025167756344\n",
      "6.920567015736349\n",
      "6.859808498643032\n",
      "6.801605311095764\n",
      "6.745821590877673\n",
      "6.692329393653343\n",
      "6.641008201875795\n",
      "6.591744465580722\n",
      "6.544431172925866\n",
      "6.498967448480234\n",
      "6.455258177405706\n",
      "6.413213653801086\n",
      "6.37274925159737\n",
      "6.333785116503487\n",
      "6.296245877604434\n",
      "6.260060377309333\n",
      "6.225161418435477\n",
      "6.191485527297622\n",
      "6.1589727317483005\n",
      "6.127566353186437\n",
      "6.0972128116186095\n",
      "6.067861442918593\n",
      "6.0394643274892\n",
      "6.011976129583561\n",
      "5.985353946593303\n",
      "5.959557167657386\n",
      "5.934547340989042\n",
      "5.91028804935805\n",
      "5.886744793203762\n",
      "5.863884880888752\n",
      "5.8416773256360175\n",
      "5.820092748722675\n",
      "5.799103288531552\n",
      "5.7786825150885335\n",
      "5.758805349737765\n",
      "5.739447989630188\n",
      "5.720587836721716\n",
      "5.702203430997768\n",
      "5.684274387658938\n",
      "5.666781338020265\n",
      "5.649705873892602\n",
      "5.633030495229451\n",
      "5.616738560836908\n",
      "5.600814241957326\n",
      "5.585242478549644\n",
      "5.5700089381005276\n",
      "5.555099976811414\n",
      "5.540502603016068\n",
      "5.526204442693164\n",
      "5.512193706946276\n",
      "5.498459161332481\n",
      "5.484990096927948\n",
      "5.471776303025832\n",
      "5.458808041368943\n",
      "5.446076021825111\n",
      "5.433571379419409\n",
      "5.421285652642705\n",
      "5.409210762960935\n",
      "5.397338995454084\n",
      "5.385662980518791\n",
      "5.3741756765718\n",
      "5.3628703536961275\n",
      "5.351740578174902\n",
      "5.340780197861275\n",
      "5.329983328336288\n",
      "5.319344339809023\n",
      "5.30885784471657\n",
      "5.298518685983696\n",
      "5.288321925904523\n",
      "5.2782628356110255\n",
      "5.268336885094876\n",
      "5.258539733751561\n",
      "5.248867221417435\n",
      "5.23931535987189\n",
      "5.22988032477883\n",
      "5.220558448042962\n",
      "5.211346210557892\n",
      "5.202240235324387\n",
      "5.193237280918394\n",
      "5.184334235289749\n",
      "5.17552810987333\n",
      "5.166816033995821\n",
      "5.158195249561921\n",
      "5.149663106005\n",
      "5.141217055487853\n",
      "5.1328546483402695\n",
      "5.1245735287206795\n",
      "5.116371430489913\n",
      "5.108246173285968\n",
      "5.100195658789001\n",
      "5.092217867166643\n",
      "5.08431085369016\n",
      "5.076472745512503\n",
      "5.068701738599849\n",
      "5.060996094808683\n",
      "5.053354139100923\n",
      "5.045774256889907\n",
      "5.038254891510667\n",
      "5.030794541808045\n",
      "5.023391759836688\n",
      "5.016045148667296\n",
      "5.008753360293669\n",
      "5.001515093635694\n",
      "4.994329092633129\n",
      "4.98719414442604\n",
      "4.980109077617347\n",
      "4.9730727606134355\n",
      "4.966084100039129\n",
      "4.959142039223236\n",
      "4.9522455567512305\n",
      "4.945393665081959\n",
      "4.9385854092249994\n",
      "4.931819865476066\n",
      "4.925096140207342\n",
      "4.918413368710405\n",
      "4.91177071408898\n",
      "4.9051673661994055\n",
      "4.898602540636324\n",
      "4.8920754777616215\n",
      "4.885585441774501\n",
      "4.879131719820876\n",
      "4.872713621140057\n",
      "4.86633047624728\n",
      "4.859981636150158\n",
      "4.853666471597609\n",
      "4.847384372359875\n",
      "4.841134746538027\n",
      "4.834917019901689\n",
      "4.828730635253825\n",
      "4.82257505182117\n",
      "4.816449744669288\n",
      "4.810354204141144\n",
      "4.804287935318068\n",
      "4.798250457502262\n",
      "4.792241303719727\n",
      "4.786260020242831\n",
      "4.780306166131681\n",
      "4.774379312793365\n",
      "4.7684790435583375\n",
      "4.762604953273314\n",
      "4.756756647909788\n",
      "4.750933744187657\n",
      "4.745135869213158\n",
      "4.73936266013069\n",
      "4.733613763787816\n",
      "4.727888836412798\n",
      "4.722187543304471\n",
      "4.716509558533537\n",
      "4.71085456465504\n",
      "4.705222252431602\n",
      "4.699612320566779\n",
      "4.694024475448235\n",
      "4.688458430900418\n",
      "4.682913907946171\n",
      "4.677390634577045\n",
      "4.671888345531887\n",
      "4.6664067820834685\n",
      "4.660945691832725\n",
      "4.655504828510341\n",
      "4.650083951785446\n",
      "4.644682827081039\n",
      "4.6393012253959265\n",
      "4.63393892313304\n",
      "4.628595701933554\n",
      "4.623271348516989\n",
      "4.617965654526756\n",
      "4.612678416381075\n",
      "4.607409435129071\n",
      "4.602158516311727\n",
      "4.59692546982767\n",
      "4.591710109803492\n",
      "4.586512254468486\n",
      "4.581331726033623\n",
      "4.576168350574573\n",
      "4.571021957918661\n",
      "4.565892381535661\n",
      "4.56077945843212\n",
      "4.555683029049306\n",
      "4.550602937164429\n",
      "4.545539029795162\n",
      "4.540491157107315\n",
      "4.535459172325485\n",
      "4.530442931646626\n",
      "4.525442294156462\n",
      "4.520457121748564\n",
      "4.515487279046028\n",
      "4.510532633325744\n",
      "4.505593054444964\n",
      "4.50066841477033\n",
      "4.495758589109121\n",
      "4.490863454642639\n",
      "4.485982890861715\n",
      "4.481116779504308\n",
      "4.476265004494985\n",
      "4.471427451886297\n",
      "4.466604009802037\n",
      "4.461794568382193\n",
      "4.456999019729697\n",
      "4.4522172578586625\n",
      "4.447449178644397\n",
      "4.442694679774804\n",
      "4.437953660703347\n",
      "4.433226022603416\n",
      "4.428511668324086\n",
      "4.4238105023471945\n",
      "4.4191224307457695\n",
      "4.414447361143646\n",
      "4.4097852026763\n",
      "4.405135865952919\n",
      "4.400499263019528\n",
      "4.395875307323212\n",
      "4.391263913677458\n",
      "4.3866649982284525\n",
      "4.382078478422461\n",
      "4.377504272974019\n",
      "4.372942301835204\n",
      "4.368392486165723\n",
      "4.363854748303861\n",
      "4.359329011738353\n",
      "4.354815201080919\n",
      "4.350313242039818\n",
      "4.345823061393869\n",
      "4.341344586967482\n",
      "4.336877747606289\n",
      "4.3324224731534\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    theta = gradient_descent(theta, X_norm, y, alpha, m)\n",
    "    print(loss_function(theta, X_norm, y, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Test Our Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      110.0  1.0    0.0    240.0  0.0    23.0   2.0    30.0   25.0   1.13   41.998 \n",
      "\n",
      "1      110.0  2.0    0.0    290.0  0.0    22.0   3.0    35.0   25.0   1.0    40.560 \n",
      "\n",
      "1      96.385 2.4096 0.0    0.0    3.6144 19.277 0.0    114.45 0.0    1.2048 68.235 \n",
      "\n",
      "1      90.0   3.0    0.0    0.0    4.0    19.0   0.0    140.0  0.0    0.67   74.472 \n",
      "\n",
      "1      90.0   3.0    0.0    0.0    3.0    20.0   0.0    120.0  0.0    0.67   72.801 \n",
      "\n",
      "1      110.0  2.0    1.0    70.0   1.0    9.0    15.0   40.0   25.0   0.75   31.230 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  16 \n",
      "\n",
      "optom theta [41.392559885245895, -13.263453463650928, 13.03366630589861, -15.63095113348022, -13.21073488305321, 21.75434241159308, 11.747963851012901, -20.718339730622937, 7.958572761747221, -2.574270424022168, 1.8193371732991293]\n",
      "1      0.0560 -0.299 -0.215 0.2675 -0.151 0.3441 -0.318 -0.194 -0.004 0.1836 \n",
      "\n",
      "1      0.0560 -0.099 -0.215 0.4238 -0.151 0.3070 -0.255 -0.179 -0.004 0.1093 \n",
      "\n",
      "1      -0.080 -0.017 -0.215 -0.482 0.1062 0.2062 -0.443 0.0613 -0.254 0.2263 \n",
      "\n",
      "1      -0.143 0.1003 -0.215 -0.482 0.1338 0.1959 -0.443 0.1387 -0.254 -0.079 \n",
      "\n",
      "1      -0.143 0.1003 -0.215 -0.482 0.0623 0.2329 -0.443 0.0781 -0.254 -0.079 \n",
      "\n",
      "1      0.0560 -0.099 -0.015 -0.263 -0.080 -0.174 0.4944 -0.164 -0.004 -0.033 \n",
      "\n",
      "** Total Number of columns:  16\n",
      "** Displaying rows 1 -  6  of  16 \n",
      "\n",
      "yvalidation:  [41.998933, 40.560159, 68.235885, 74.472949, 72.801787, 31.230054, 53.131324, 59.363993, 38.839746, 28.592785, 46.658844, 39.106174, 27.753301, 49.787445, 51.592193, 36.187559]\n",
      "predicted  actual    difference\n",
      "42.706     41.998    0.7075\n",
      "41.504     40.560    0.9442\n",
      "67.439     68.235    -0.796\n",
      "70.363     74.472    -4.109\n",
      "68.762     72.801    -4.039\n",
      "27.680     31.230    -3.550\n",
      "53.833     53.131    0.7026\n",
      "55.862     59.363    -3.501\n",
      "39.727     38.839    0.8882\n",
      "40.529     28.592    11.937\n",
      "47.955     46.658    1.2970\n",
      "39.937     39.106    0.8313\n",
      "26.152     27.753    -1.600\n",
      "48.860     49.787    -0.926\n",
      "50.321     51.592    -1.270\n",
      "34.905     36.187    -1.282\n"
     ]
    }
   ],
   "source": [
    "peak_in(validation_set)\n",
    "\n",
    "X_validation = []\n",
    "y_validation = []\n",
    "\n",
    "print('optom theta', theta)\n",
    "\n",
    "for row in validation_set:\n",
    "    X_validation.append(row[:11])\n",
    "    y_validation.append(row[11])\n",
    "    \n",
    "X_validation_norm = normalize(X_validation)\n",
    "\n",
    "peak_in(X_validation_norm)\n",
    "\n",
    "print('yvalidation: ', y_validation)\n",
    "\n",
    "print('predicted ', 'actual   ', 'difference')\n",
    "for row_num, row in enumerate(X_validation_norm):\n",
    "    predicted_score = hypothesis(theta, X_validation_norm[row_num])\n",
    "    actual_score = y_validation[row_num]\n",
    "    difference = predicted_score - actual_score\n",
    "    print(str(predicted_score)[:6], '   ',\n",
    "          str(actual_score)[:6], '  ', \n",
    "          str(difference)[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it in 5 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan    nan    nan    nan    nan    nan    nan    nan    nan    nan    nan    nan    nan    \n",
      "\n",
      "70.0   4.0    1.0    130.0  10.0   5.0    6.0    280.0  25.0   3.0    1.0    0.33   68.402 \n",
      "\n",
      "120.0  3.0    5.0    15.0   2.0    8.0    8.0    135.0  0.0    3.0    1.0    1.0    33.983 \n",
      "\n",
      "70.0   4.0    1.0    260.0  9.0    7.0    5.0    320.0  25.0   3.0    1.0    0.33   59.425 \n",
      "\n",
      "50.0   4.0    0.0    140.0  14.0   8.0    0.0    330.0  25.0   3.0    1.0    0.5    93.704 \n",
      "\n",
      "110.0  2.0    2.0    200.0  1.0    14.0   8.0    0.0    25.0   3.0    1.0    0.75   34.384 \n",
      "\n",
      "** Total Number of columns:  13\n",
      "** Displaying rows 1 -  6  of  78 \n",
      "\n",
      "[[ 80.     2.     0.     0.     3.    16.     0.    95.     0.     1.\n",
      "    0.83   1.  ]\n",
      " [120.     1.     2.   220.     1.    12.    11.    45.    25.     2.\n",
      "    1.     1.  ]\n",
      " [100.     3.     1.   140.     3.    15.     5.    85.    25.     3.\n",
      "    1.     0.88]\n",
      " [130.     3.     2.   210.     2.    18.     8.   100.    25.     3.\n",
      "    1.33   0.75]\n",
      " [ 50.     4.     0.   140.    14.     8.     0.   330.    25.     3.\n",
      "    1.     0.5 ]\n",
      " [110.     2.     1.    70.     1.     9.    15.    40.    25.     2.\n",
      "    1.     0.75]\n",
      " [ 50.     2.     0.     0.     1.    10.     0.    50.     0.     3.\n",
      "    0.5    1.  ]\n",
      " [110.     3.     1.   250.     1.5   11.5   10.    90.    25.     1.\n",
      "    1.     0.75]\n",
      " [110.     2.     0.   220.     1.    21.     3.    30.    25.     3.\n",
      "    1.     1.  ]\n",
      " [110.     1.     1.   180.     0.    12.    13.    65.    25.     2.\n",
      "    1.     1.  ]\n",
      " [100.     2.     1.   220.     2.    15.     6.    90.    25.     1.\n",
      "    1.     1.  ]\n",
      " [100.     2.     1.   140.     2.    11.    10.   120.    25.     3.\n",
      "    1.     0.75]\n",
      " [110.     1.     1.   180.     0.    12.    13.    55.    25.     2.\n",
      "    1.     1.  ]\n",
      " [100.     2.     0.    45.     0.    11.    15.    40.    25.     1.\n",
      "    1.     0.88]\n",
      " [ 50.     1.     0.     0.     0.    13.     0.    15.     0.     3.\n",
      "    0.5    1.  ]\n",
      " [120.     3.     1.   210.     5.    14.    12.   240.    25.     2.\n",
      "    1.33   0.75]\n",
      " [100.     4.     1.   135.     2.    14.     6.   110.    25.     3.\n",
      "    1.     0.5 ]\n",
      " [120.     3.     5.    15.     2.     8.     8.   135.     0.     3.\n",
      "    1.     1.  ]\n",
      " [ 70.     4.     1.   260.     9.     7.     5.   320.    25.     3.\n",
      "    1.     0.33]\n",
      " [100.     5.     2.     0.     2.7   -1.    -1.   110.     0.     1.\n",
      "    1.     0.67]\n",
      " [100.     3.     0.   320.     1.    20.     3.    45.   100.     3.\n",
      "    1.     1.  ]\n",
      " [ 70.     4.     1.   130.    10.     5.     6.   280.    25.     3.\n",
      "    1.     0.33]\n",
      " [110.     1.     0.   180.     0.    14.    11.    35.    25.     1.\n",
      "    1.     1.33]\n",
      " [110.     3.     3.   140.     4.    10.     7.   160.    25.     3.\n",
      "    1.     0.5 ]\n",
      " [120.     2.     1.   190.     0.    15.     9.    40.    25.     2.\n",
      "    1.     0.67]\n",
      " [110.     2.     0.   125.     1.    11.    14.    30.    25.     2.\n",
      "    1.     1.  ]\n",
      " [ 90.     2.     1.   200.     4.    15.     6.   125.    25.     1.\n",
      "    1.     0.67]\n",
      " [110.     2.     1.   180.     0.    12.    12.    55.    25.     2.\n",
      "    1.     1.  ]\n",
      " [ 90.     3.     0.     0.     3.    20.     0.   120.     0.     1.\n",
      "    1.     0.67]\n",
      " [100.     2.     0.   190.     1.    18.     5.    80.    25.     3.\n",
      "    1.     0.75]\n",
      " [110.     6.     0.   230.     1.    16.     3.    55.    25.     1.\n",
      "    1.     1.  ]\n",
      " [100.     3.     2.   140.     2.5   10.5    8.   140.    25.     3.\n",
      "    1.     0.5 ]\n",
      " [110.     2.     1.   200.     1.    16.     8.    60.    25.     1.\n",
      "    1.     0.75]\n",
      " [100.     4.     1.     0.     0.    16.     3.    95.    25.     2.\n",
      "    1.     1.  ]\n",
      " [120.     3.     3.    75.     3.    13.     4.   100.    25.     3.\n",
      "    1.     0.33]\n",
      " [110.     2.     0.   280.     0.    22.     3.    25.    25.     1.\n",
      "    1.     1.  ]\n",
      " [110.     1.     1.   280.     0.    15.     9.    45.    25.     2.\n",
      "    1.     0.75]\n",
      " [100.     2.     0.   290.     1.    21.     2.    35.    25.     1.\n",
      "    1.     1.  ]\n",
      " [110.     1.     1.   140.     0.    13.    12.    25.    25.     2.\n",
      "    1.     1.  ]\n",
      " [110.     1.     1.   135.     0.    13.    12.    25.    25.     2.\n",
      "    1.     0.75]\n",
      " [110.     1.     0.   200.     1.    14.    11.    25.    25.     1.\n",
      "    1.     0.75]\n",
      " [ 90.     3.     0.     0.     4.    19.     0.   140.     0.     1.\n",
      "    1.     0.67]\n",
      " [110.     2.     2.   180.     1.5   10.5   10.    70.    25.     1.\n",
      "    1.     0.75]\n",
      " [110.     2.     1.   125.     1.    11.    13.    30.    25.     2.\n",
      "    1.     1.  ]\n",
      " [120.     3.     0.   240.     5.    14.    12.   190.    25.     3.\n",
      "    1.33   0.67]\n",
      " [110.     2.     1.   170.     1.    17.     6.    60.   100.     3.\n",
      "    1.     1.  ]\n",
      " [110.     2.     1.   260.     0.    21.     3.    40.    25.     2.\n",
      "    1.     1.5 ]\n",
      " [110.     2.     2.   200.     1.    14.     8.     0.    25.     3.\n",
      "    1.     0.75]\n",
      " [150.     4.     3.    95.     3.    16.    11.   170.    25.     3.\n",
      "    1.     1.  ]\n",
      " [140.     3.     2.   220.     3.    21.     7.   130.    25.     3.\n",
      "    1.33   0.67]\n",
      " [100.     3.     1.   200.     3.    17.     3.   110.    25.     1.\n",
      "    1.     1.  ]\n",
      " [160.     3.     2.   150.     3.    17.    13.   160.    25.     3.\n",
      "    1.5    0.67]\n",
      " [130.     3.     2.   170.     1.5   13.5   10.   120.    25.     3.\n",
      "    1.25   0.5 ]\n",
      " [140.     3.     1.   170.     2.    20.     9.    95.   100.     3.\n",
      "    1.3    0.75]\n",
      " [120.     1.     3.   210.     0.    13.     9.    45.    25.     2.\n",
      "    1.     0.75]\n",
      " [110.     1.     0.   240.     0.    23.     2.    30.    25.     1.\n",
      "    1.     1.13]\n",
      " [110.     6.     2.   290.     2.    17.     1.   105.    25.     1.\n",
      "    1.     1.25]\n",
      " [110.     2.     1.   200.     0.    21.     3.    35.   100.     3.\n",
      "    1.     1.  ]]\n",
      "reg.coef_ [-2.22852082e-01  3.27408722e+00 -1.68955808e+00 -5.44876919e-02\n",
      "  3.44502071e+00  1.09266954e+00 -7.24639356e-01 -3.40831100e-02\n",
      " -5.12513948e-02  1.12658424e-03  1.07028511e-02 -1.27656008e-03]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-957d764d38b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reg.coef_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m    241\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, linear_model\n",
    "import numpy\n",
    "\n",
    "\n",
    "data = numpy.genfromtxt('cereal.csv', delimiter=',')[1:,3:]\n",
    "peak_in(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data[:,:12],data[:,12])\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit (X_train, y_train)\n",
    "print('reg.coef_', reg.coef_)\n",
    "print('predict', reg.predict(X_test))\n",
    "print('actual', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
